{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To run first place Posts_Coffee.xml, and post_parser_record.py in session storage, then create a folder in session storage named Entities and place Post.py inside, then run code sections in order"
      ],
      "metadata": {
        "id": "bW-qdfpF8ABp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M3H-HvR765D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "969089f1-c870-4da4-d8dd-fd407ba7b1c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from post_parser_record import PostParserRecord\n",
        "post_reader = PostParserRecord(\"Posts_Coffee.xml\")\n",
        "!pip3 install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates Inverted Index"
      ],
      "metadata": {
        "id": "WL7cndld9B0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This section is to prepare the dictionary dic, and use tokens to assign each \n",
        "#document id with the ammount of occurences to a specific token\n",
        "#example dic will contain an inverted index such as {\"espresso\": {1: 4}, {2: 1}}  \n",
        "#if espresso in document 1 had 4 matches, and 1 match in document 2\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "#print(stopwords.words('english'))\n",
        "\n",
        "#initializes variables \n",
        "recall = []\n",
        "precision = []\n",
        "matches_found = 0;\n",
        "i= 0\n",
        "values = {}\n",
        "dic = {}\n",
        "\n",
        "#discovered how to use functions in python!\n",
        "def tokenize(tokens, id, dic):\n",
        "  #goes through each token \n",
        "  for word in tokens: \n",
        "    #stopwords shall not pass\n",
        "    if word not in stop_words:\n",
        "      #if not in dic, it creates a new entry and gives it one occurrence\n",
        "      if word not in dic: \n",
        "        dic[word] = {id:1}\n",
        "      else:\n",
        "        #else if word exists in dic, but id does not, creates a new entry and gives it one occurrence\n",
        "        if id not in dic[word]: \n",
        "          dic[word][id] = 1\n",
        "        else:\n",
        "          #else the token and id allready exists in dic, so occurrence++\n",
        "          dic[word][id] += 1\n",
        "\n",
        "#runs through every question post\n",
        "for question_id in post_reader.map_questions:\n",
        "  #tokenizes for questions\n",
        "  question = post_reader.map_questions[question_id]\n",
        "  tokens = word_tokenize(re.sub(r'[^a-z]', ' ', question.title.lower()) + re.sub(r'[^a-z]', ' ', question.body.lower()))\n",
        "  tokenize(tokens, question_id, dic)\n",
        "\n",
        "#runs through every answer post\n",
        "for answer_id in post_reader.map_just_answers:\n",
        "  #tokenizes for questions\n",
        "  answer = post_reader.map_just_answers[answer_id]\n",
        "  tokens = word_tokenize(re.sub(r'[^a-z]', ' ', answer.body.lower()))\n",
        "  tokenize(tokens, answer_id, dic)\n",
        "\n"
      ],
      "metadata": {
        "id": "FbYBky2X8CJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run to simply initalize this function for use \n",
        "def search(keys,words,query_num):\n",
        "  temp_dic = {}\n",
        "  \n",
        "  seperated_words = words.split(\" \")\n",
        "  #checks that the query is larger than one word\n",
        "  if len(seperated_words) is not 1:\n",
        "    #ensures full query is valid\n",
        "    for word in seperated_words:  \n",
        "      if word not in dic:\n",
        "        return \"\"\n",
        "      #scans through each key \n",
        "      for current_key in keys:\n",
        "        bad_match = False\n",
        "        #checks if key is held for all words of query\n",
        "################################################################################\n",
        "        for word in seperated_words:\n",
        "          if int(str(current_key)) not in dic[word]:\n",
        "            bad_match = True\n",
        "        if(bad_match == False):\n",
        "          temp_val = 0\n",
        "          for word in seperated_words:\n",
        "            temp_val += dic[word][current_key]\n",
        "          temp_dic[current_key] = temp_val\n",
        "################################################################################\n",
        "  #query is only one word\n",
        "  else:\n",
        "    for current_key in keys: \n",
        "      #simply copys the dictionary of specific word to temp_dic\n",
        "      temp_dic = dic.copy()[words]\n",
        "  query_num+=1 \n",
        "\n",
        "  #reverse sorts, by largest amount of occurrence first\n",
        "  myList = sorted(temp_dic.items(), key=lambda x: x[1], reverse=True)\n",
        "  i = 0\n",
        "  for items in myList:\n",
        "    #makes sure only 10 of each pass are printed\n",
        "    if i <= 9:\n",
        "      print(str(items[0])+\"    \"+str(items[1]))\n",
        "      i += 1\n",
        "    else:\n",
        "      break"
      ],
      "metadata": {
        "id": "GRgIXL1IlK25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Querys\n",
        "#simply add Query to list \n",
        "querys = [\"espresso\", \"turkish coffee\",\"persian coffee\"]\n",
        "keys = []\n",
        "query_num = 0\n",
        "\n",
        "#scans through and prints each query\n",
        "for words in querys:\n",
        "  print(words)\n",
        "  #seperates query into a list\n",
        "  seperated_query = words.split(\" \")\n",
        "  #scans through each word in query specifically to save all matching occurances to the list of keys\n",
        "  for query_part in seperated_query:\n",
        "    if query_part in dic:\n",
        "      #adds keys of matching documents for current word in query\n",
        "      keys = keys + list(dic[query_part].keys())\n",
        "      #sorts keys to keep organization for my sanity\n",
        "  keys.sort()\n",
        "  ordered_keys = []\n",
        "  #removes duplicate keys\n",
        "  [ordered_keys.append(x) for x in keys if x not in ordered_keys]\n",
        "  #uses function to index and print final result\n",
        "  search(ordered_keys,words,query_num)\n",
        "  query_num += 1\n",
        "  keys = []\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA35fQdbF2UL",
        "outputId": "a71b5437-95c6-4493-a132-5d6f3d58b160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "espresso\n",
            "3269    34\n",
            "1574    27\n",
            "2095    22\n",
            "283    17\n",
            "2077    14\n",
            "5537    14\n",
            "1651    13\n",
            "2087    12\n",
            "2116    12\n",
            "3721    12\n",
            "turkish coffee\n",
            "2392    70\n",
            "3710    30\n",
            "4407    22\n",
            "3101    20\n",
            "4185    20\n",
            "1545    19\n",
            "1833    19\n",
            "4273    19\n",
            "165    18\n",
            "2394    16\n",
            "persian coffee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "espresso\n",
        "\n",
        "ID:      Precision\n",
        "\n",
        "3269:    1\n",
        "\n",
        "1574:    1\n",
        "\n",
        "2095:    1\n",
        "\n",
        "283:     .75\n",
        "\n",
        "2077:    .8\n",
        "\n",
        "5537:    .83\n",
        "\n",
        "1651:    .85\n",
        "\n",
        "2087:    .87\n",
        "\n",
        "2116:    .88\n",
        "\n",
        "3721:    .9\n",
        "\n",
        "turkish coffee\n",
        "\n",
        "ID: Precision\n",
        "\n",
        "2392:    1\n",
        "\n",
        "3710:    .5\n",
        "\n",
        "4407:   .66\n",
        "\n",
        "3101:    .5\n",
        "\n",
        "4185:    .4\n",
        "\n",
        "1545:    .33\n",
        "\n",
        "1833:    .42\n",
        "\n",
        "4273:    .37\n",
        "\n",
        "165:     .33\n",
        "\n",
        "2394:    .3\n",
        "\n",
        "persian coffee\n",
        "\n",
        "N/A\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8k4qmp1xnBEJ"
      }
    }
  ]
}