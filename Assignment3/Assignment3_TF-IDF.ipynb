{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (2022.9.13)\n",
      "Requirement already satisfied: joblib in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nicdr_nactbrj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nicdr_nactbrj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from post_parser_record import PostParserRecord\n",
    "post_reader = PostParserRecord(\"Posts_Coffee.xml\")\n",
    "!pip3 install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves nessisary information from documents for calcumating TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexing questions...\n",
      "complete\n",
      "indexing answers...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "#creating index\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('p')\n",
    "\n",
    "inverted_index = {}\n",
    "word_count_index = {} #{document_id,word_count}\n",
    "total_document_count = 0\n",
    "#function to update the index needed for TF-IDF\n",
    "def update_index(tokens, document_id):\n",
    "  global total_document_count\n",
    "  total_document_count += 1\n",
    "  \n",
    "  word_count_index[document_id] = len(tokens)\n",
    "  #runs through all tokens and indexed accordingly\n",
    "  for token in tokens:\n",
    "    #if token not in stopwords.words('english'):\n",
    "      if token in inverted_index:\n",
    "        if document_id in inverted_index[token]:\n",
    "          inverted_index[token][document_id] += 1\n",
    "        else:\n",
    "          inverted_index[token][document_id] = 1\n",
    "      else:\n",
    "        inverted_index[token] = {document_id: 1}\n",
    "\n",
    "#functions to create tokens\n",
    "def create_tokens_for_text(text):\n",
    "  temp_tokens = word_tokenize(re.sub(r'[^\\w\\s]', ' ', text))\n",
    "  tokens = [i for i in temp_tokens if i not in stop_words]\n",
    "  return tokens\n",
    "\n",
    "#indexes questions\n",
    "print(\"indexing questions...\")\n",
    "for question_id in post_reader.map_questions:\n",
    "  question = post_reader.map_questions[question_id]\n",
    "  title = question.title.lower().strip()\n",
    "  body = question.body.lower().strip()\n",
    "  text = title + \" \" + body\n",
    "  tokens = create_tokens_for_text(text)\n",
    "  update_index(tokens, question_id)\n",
    "  \n",
    "print(\"complete\")\n",
    "\n",
    "#indexes answers\n",
    "print(\"indexing answers...\")\n",
    "for answer_id in post_reader.map_just_answers:\n",
    "  answer = post_reader.map_just_answers[answer_id]\n",
    "  text = answer.body.lower().strip()\n",
    "  tokens = create_tokens_for_text(text)\n",
    "  update_index(tokens, answer_id)\n",
    "  \n",
    "print(\"Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates TF-IDF for chosen querys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching: espresso\n",
      "Document: 4404   Score: 0.21116651151731888\n",
      "Document: 4175   Score: 0.18204009613561972\n",
      "Document: 2867   Score: 0.17597209293109906\n",
      "Document: 3168   Score: 0.17597209293109906\n",
      "Document: 3904   Score: 0.17597209293109906\n",
      "Searching: turkish coffee\n",
      "Document: 5094   Score: 0.7134110891217438\n",
      "Document: 5182   Score: 0.6153617051246203\n",
      "Document: 2522   Score: 0.6089518241789702\n",
      "Document: 483   Score: 0.4733551577881695\n",
      "Document: 209   Score: 0.4102411367497469\n",
      "Searching: making a decaffeinated coffee\n",
      "Document: 204   Score: 0.7342729167619662\n",
      "Document: 3293   Score: 0.46727202514212884\n",
      "Document: 120   Score: 0.4213748516426526\n",
      "Document: 2897   Score: 0.4005125000519816\n",
      "Document: 373   Score: 0.26701258579550213\n",
      "Searching: can i use the same coffee grounds twice?\n",
      "Document: 2683   Score: 0.5339452613665286\n",
      "Document: 1749   Score: 0.36111912934403323\n",
      "Document: 3258   Score: 0.318020209424038\n",
      "Document: 3966   Score: 0.30179366458577406\n",
      "Document: 3818   Score: 0.24360349798928144\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "#run to simply initalize this function for use \n",
    "from re import A\n",
    "import math\n",
    "#searches index for document matches\n",
    "def TFIDF(query,query_num):\n",
    "  tfidf = {}\n",
    "  tf = 0\n",
    "  for word in query:\n",
    "    total_word_count = 0\n",
    "    #counts total words\n",
    "    for document in inverted_index[word]:\n",
    "      total_word_count += inverted_index[word][document]\n",
    "    #continues\n",
    "    for document in inverted_index[word]:\n",
    "      if document not in tfidf:\n",
    "        tfidf[document] = (inverted_index[word][document]/word_count_index[document]) * (math.log(((1+total_document_count)/(1+total_word_count))+1))\n",
    "      else:\n",
    "        tfidf[document] += (inverted_index[word][document]/word_count_index[document]) * (math.log(((1+total_document_count)/(1+total_word_count))+1))\n",
    "  result = sorted(tfidf.items(), key=lambda x: x[1], reverse=True)\n",
    "  i = 0\n",
    "  for items in result:\n",
    "    #makes sure only 5 of each are printed\n",
    "    if i <= 4:\n",
    "      print(\"Document: \"+str(items[0])+\"\\tScore: \"+str(items[1]))\n",
    "      i += 1\n",
    "    else:\n",
    "      break\n",
    "################################################################################\n",
    "#Querys\n",
    "querys = [\"espresso\",\"turkish coffee\",\"making a decaffeinated coffee\",\"can I use the same coffee grounds twice?\"]\n",
    "\n",
    "query_num = 0\n",
    "#scans through and prints each query\n",
    "for temp_query in querys:\n",
    "  temp_query = temp_query.lower().strip()\n",
    "  temp_tokens = word_tokenize(re.sub(r'[^\\w\\s]', '', temp_query))\n",
    "  query_tokens = [i for i in temp_tokens if i not in stop_words]\n",
    "\n",
    "  print(\"Searching: \"+temp_query)\n",
    "  #uses function to search index and print final result\n",
    "  TFIDF(query_tokens,query_num)\n",
    "  query_num += 1\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75e7424a951a4d56918a131ca4a3853478fa8f052bd41190059678d885c50057"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
