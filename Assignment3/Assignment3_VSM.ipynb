{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.9.2)\n",
      "Requirement already satisfied: numpy<1.26.0,>=1.18.5 in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scipy) (1.23.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (2022.9.13)\n",
      "Requirement already satisfied: joblib in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nicdr_nactbrj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nicdr_nactbrj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from post_parser_record import PostParserRecord\n",
    "post_reader = PostParserRecord(\"Posts_Coffee.xml\")\n",
    "!pip3 install scipy\n",
    "!pip3 install nltk\n",
    "from scipy import spatial\n",
    "from scipy.spatial import distance\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term list created\n",
      "search for: espresso\n",
      "Document: 2766\tScore: 0.6614378277661477\n",
      "Document: 4175\tScore: 0.6401843996644798\n",
      "Document: 26\tScore: 0.6299407883487119\n",
      "Document: 5528\tScore: 0.6155870112510924\n",
      "Document: 3168\tScore: 0.588348405414552\n",
      "search for: turkish coffee\n",
      "Document: 5094\tScore: 0.7715167498104595\n",
      "Document: 2522\tScore: 0.7252406676228422\n",
      "Document: 3074\tScore: 0.7071067811865475\n",
      "Document: 2379\tScore: 0.6832312780114154\n",
      "Document: 5095\tScore: 0.6415415518117004\n",
      "search for: making a decaffeinated coffee\n",
      "Document: 120\tScore: 0.560448538317805\n",
      "Document: 4193\tScore: 0.501280411827603\n",
      "Document: 3293\tScore: 0.5\n",
      "Document: 204\tScore: 0.492365963917331\n",
      "Document: 2158\tScore: 0.49170515840695195\n",
      "search for: can i use the same coffee grounds twice?\n",
      "Document: 2683\tScore: 0.6565321642986127\n",
      "Document: 1749\tScore: 0.6002450479987808\n",
      "Document: 3258\tScore: 0.5454545454545454\n",
      "Document: 5121\tScore: 0.5388159060803247\n",
      "Document: 2609\tScore: 0.5151021148075838\n"
     ]
    }
   ],
   "source": [
    "#creating index\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('p')\n",
    "\n",
    "#creates tokens\n",
    "def create_tokens_for_text(text):\n",
    "  temp_tokens = word_tokenize(re.sub(r'[^\\w\\s]', ' ', text))\n",
    "  tokens = [i for i in temp_tokens if i not in stop_words]\n",
    "  return tokens\n",
    "\n",
    "#finds and returns all terms in document\n",
    "def get_list_terms():\n",
    "    list_terms = []\n",
    "    for answer_id in post_reader.map_just_answers:\n",
    "      answer = post_reader.map_just_answers[answer_id]\n",
    "      text = answer.body.lower().strip()\n",
    "      split_words = create_tokens_for_text(text)\n",
    "      list_terms.extend(split_words)\n",
    "\n",
    "    for question_id in post_reader.map_questions:\n",
    "      question = post_reader.map_questions[question_id]\n",
    "      title = question.title.lower().strip()\n",
    "      body = question.body.lower().strip()\n",
    "      text = title + \" \" + body\n",
    "      split_words = create_tokens_for_text(text)\n",
    "      list_terms.extend(split_words)\n",
    "    return list_terms\n",
    "\n",
    "list_terms = get_list_terms()\n",
    "print(\"term list created\")\n",
    "#print(str(len(list_terms)))\n",
    "#print(str(len(set(list_terms))))\n",
    "\n",
    "temp_dic = {}\n",
    "count = 0\n",
    "\n",
    "#creates a dictionary of the terms with the position in the index\n",
    "for term in set(list_terms):\n",
    "  if term not in set(temp_dic):\n",
    "    temp_dic[term] = count\n",
    "    count +=1\n",
    "\n",
    "######################################Document############################################\n",
    "dic_doc_vector = {}\n",
    "query_results = {}\n",
    "\n",
    "#creates document vector for answers\n",
    "for answer_id in post_reader.map_just_answers:\n",
    "  answer = post_reader.map_just_answers[answer_id]\n",
    "  doc_vector = [0] *len(temp_dic.keys())\n",
    "  text = answer.body.lower().strip()\n",
    "  filtered_words = create_tokens_for_text(text)\n",
    "  for token in filtered_words:\n",
    "    if token in temp_dic:\n",
    "      index = temp_dic[token]\n",
    "      doc_vector[index] += 1\n",
    "  dic_doc_vector[answer_id] = doc_vector\n",
    "\n",
    "#creates document vector for questions\n",
    "for question_id in post_reader.map_questions:\n",
    "  question = post_reader.map_questions[question_id]\n",
    "  doc_vector = [0] *len(temp_dic.keys())\n",
    "  title = question.title.lower().strip()\n",
    "  body = question.body.lower().strip()\n",
    "  text = title + \" \" + body\n",
    "  filtered_words = create_tokens_for_text(text)\n",
    "  for token in filtered_words:\n",
    "    if token in temp_dic:\n",
    "      index = temp_dic[token]\n",
    "      doc_vector[index] += 1\n",
    "  dic_doc_vector[question_id] = doc_vector\n",
    "######################################Document############################################\n",
    "\n",
    "######################################querys############################################\n",
    "dic_query_vector = {}\n",
    "querys = [\"espresso\",\"turkish coffee\",\"making a decaffeinated coffee\",\"can I use the same coffee grounds twice?\"]\n",
    "\n",
    "#marks query for printing qurel or runfile\n",
    "query_num = 0\n",
    "\n",
    "#creates document vector for questions\n",
    "for temp_query in querys:\n",
    "  temp_query = temp_query.lower().strip()\n",
    "  temp_tokens = word_tokenize(re.sub(r'[^\\w\\s]', '', temp_query))\n",
    "  query_tokens = [i for i in temp_tokens if i not in stop_words]\n",
    "  print(\"search for: \"+str(temp_query))\n",
    "\n",
    "  #creates vector for query of answers\n",
    "  for answer_id in post_reader.map_just_answers:\n",
    "    answer = post_reader.map_just_answers[answer_id]\n",
    "    query_vector = [0] *len(temp_dic.keys())\n",
    "    text = answer.body.lower().strip()\n",
    "    filtered_words = create_tokens_for_text(text)\n",
    "    for token in query_tokens:\n",
    "      if token in temp_dic:\n",
    "        index = temp_dic[token]\n",
    "        query_vector[index] += 1\n",
    "    dic_query_vector[answer_id] = query_vector\n",
    "    result = 1 - distance.cosine(query_vector, dic_doc_vector[answer_id])\n",
    "    query_results[answer_id] = result\n",
    "\n",
    "  #creates vector for query of questions\n",
    "  for question_id in post_reader.map_questions:\n",
    "    question = post_reader.map_questions[question_id]\n",
    "    query_vector = [0] *len(temp_dic.keys())\n",
    "    title = question.title.lower().strip()\n",
    "    body = question.body.lower().strip()\n",
    "    text = title + \" \" + body\n",
    "    filtered_words = create_tokens_for_text(text)\n",
    "    for token in query_tokens:\n",
    "      if token in temp_dic:\n",
    "        index = temp_dic[token]\n",
    "        query_vector[index] += 1\n",
    "    dic_query_vector[question_id] = query_vector\n",
    "    result = 1 - distance.cosine(query_vector, dic_doc_vector[question_id])\n",
    "    query_results[question_id] = result\n",
    "######################################querys############################################\n",
    "\n",
    "######################################print############################################\n",
    "  result_set = sorted(query_results.items(), key=lambda x: x[1], reverse=True)\n",
    "  i = 0\n",
    "  query_num += 1\n",
    "  for items in result_set:\n",
    "    if i <= 4:\n",
    "      print(\"Document: \"+str(items[0])+\"\\tScore: \"+str(items[1]))\n",
    "      #to print qrel\n",
    "      #print(\"A\"+str(query_num)+\"\\t0\\t\"+str(items[0])+\"\\t\")\n",
    "      #to print run_file\n",
    "      #print(\"A\"+str(query_num)+\"\\tQ0\\t\"+str(items[0])+\"\\t\"+str(i+1)+\"\\t\"+str(\"%.02f\"%items[1])+\"\\tSTANDARD\")\n",
    "      i += 1\n",
    "    else:\n",
    "      break\n",
    "######################################print############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24\n"
     ]
    }
   ],
   "source": [
    "num = 1.238475930\n",
    "print(\"%.02f\"%num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75e7424a951a4d56918a131ca4a3853478fa8f052bd41190059678d885c50057"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
