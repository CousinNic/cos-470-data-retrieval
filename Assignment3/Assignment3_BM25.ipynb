{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (2022.9.13)\n",
      "Requirement already satisfied: click in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nicdr_nactbrj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nicdr_nactbrj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nicdr_nactbrj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from post_parser_record import PostParserRecord\n",
    "post_reader = PostParserRecord(\"Posts_Coffee.xml\")\n",
    "!pip3 install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexing questions...\n",
      "complete\n",
      "indexing answers...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "#creating index\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('p')\n",
    "\n",
    "inverted_index = {}\n",
    "word_count_index = {} #{document_id,word_count}\n",
    "average_doc_word_count = 0\n",
    "total_document_count = 0\n",
    "\n",
    "#updates index for each document\n",
    "def update_index(tokens, document_id):\n",
    "  global average_doc_word_count\n",
    "  global total_document_count\n",
    "  #counts total document count\n",
    "  total_document_count += 1\n",
    "  \n",
    "  average_doc_word_count += len(tokens)\n",
    "  word_count_index[document_id] = len(tokens)\n",
    "  #runs through all tokens and indexed accordingly\n",
    "  for token in tokens:\n",
    "    #if token not in stopwords.words('english'):\n",
    "      if token in inverted_index:\n",
    "        if document_id in inverted_index[token]:\n",
    "          inverted_index[token][document_id] += 1\n",
    "        else:\n",
    "          inverted_index[token][document_id] = 1\n",
    "      else:\n",
    "        inverted_index[token] = {document_id: 1}\n",
    "\n",
    "#creates tokens for given text\n",
    "def create_tokens_for_text(text):\n",
    "  temp_tokens = word_tokenize(re.sub(r'[^\\w\\s]', ' ', text))\n",
    "  tokens = [i for i in temp_tokens if i not in stop_words]\n",
    "  return tokens\n",
    "\n",
    "#indexes questions\n",
    "print(\"indexing questions...\")\n",
    "for question_id in post_reader.map_questions:\n",
    "  question = post_reader.map_questions[question_id]\n",
    "  title = question.title.lower().strip()\n",
    "  body = question.body.lower().strip()\n",
    "  text = title + \" \" + body\n",
    "  tokens = create_tokens_for_text(text)\n",
    "  update_index(tokens, question_id)\n",
    "  \n",
    "print(\"complete\")\n",
    "\n",
    "#indexes answers\n",
    "print(\"indexing answers...\")\n",
    "for answer_id in post_reader.map_just_answers:\n",
    "  answer = post_reader.map_just_answers[answer_id]\n",
    "  text = answer.body.lower().strip()\n",
    "  tokens = create_tokens_for_text(text)\n",
    "  update_index(tokens, answer_id)\n",
    "average_doc_word_count = average_doc_word_count / total_document_count\n",
    "print(\"Complete\")\n",
    "#print(inverted_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching: espresso\n",
      "Document: 3904\tScore: 2.0784213678473935\n",
      "Document: 4404\tScore: 1.8639027427470725\n",
      "Document: 95\tScore: 1.7724342457061208\n",
      "Document: 4486\tScore: 1.7724342457061208\n",
      "Document: 2950\tScore: 1.7724342457061208\n",
      "Searching: turkish coffee\n",
      "Document: 4486\tScore: 7.753714720306248\n",
      "Document: 3369\tScore: 7.39101089326847\n",
      "Document: 5182\tScore: 5.327411870482642\n",
      "Document: 2879\tScore: 4.805761773850875\n",
      "Document: 5094\tScore: 4.762993780825598\n",
      "Searching: making a decaffeinated coffee\n",
      "Document: 3225\tScore: 10.594121159754671\n",
      "Document: 3293\tScore: 8.720526258337124\n",
      "Document: 204\tScore: 8.292578269441174\n",
      "Document: 2897\tScore: 6.658736093340034\n",
      "Document: 97\tScore: 6.159593998918753\n",
      "Searching: can i use the same coffee grounds twice?\n",
      "Document: 1749\tScore: 11.011258712345983\n",
      "Document: 2683\tScore: 9.607546434699618\n",
      "Document: 3966\tScore: 9.43542627425028\n",
      "Document: 5582\tScore: 9.41040956537419\n",
      "Document: 3266\tScore: 7.801729802826547\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "#run to simply initalize this function for use \n",
    "from re import A\n",
    "import math\n",
    "#searches index for document matches\n",
    "def BM25(query,query_num):\n",
    "  bm25 = {}\n",
    "  k = 1.2\n",
    "  b = .75\n",
    "  tf = 0\n",
    "  idf = 0\n",
    "  for word in query:\n",
    "    total_word_count = 0\n",
    "    #counts total words\n",
    "    for document in inverted_index[word]:\n",
    "      total_word_count += inverted_index[word][document]\n",
    "    #continues\n",
    "    for document in inverted_index[word]:\n",
    "      #saves parts for BM25 calculation\n",
    "      tf = (inverted_index[word][document]/word_count_index[document])\n",
    "      idf = (math.log(((total_document_count)/(1+total_word_count))))\n",
    "      dl_adl = word_count_index[document]/average_doc_word_count\n",
    "\n",
    "      #uses saved values to calculate BM25\n",
    "      if document not in bm25:\n",
    "        bm25[document] = tf * idf\n",
    "        bm25[document] = idf * ((k+1*tf)/((tf+k)*(1.0 - b + (b * dl_adl))))\n",
    "      else:\n",
    "        bm25[document] += idf * ((k+1*tf)/((tf+k)*(1.0 - b + (b * dl_adl))))\n",
    "\n",
    "  #sorts results largest first\n",
    "  result = sorted(bm25.items(), key=lambda x: x[1], reverse=True)\n",
    "  i = 0\n",
    "  for items in result:\n",
    "    #makes sure only 5 of each pass are printed\n",
    "    if i <= 4:\n",
    "      print(\"Document: \"+str(items[0])+\"\\tScore: \"+str(items[1]))\n",
    "      #to print qrel\n",
    "      #print(\"A\"+str(query_num)+\"\\t0\\t\"+str(items[0])+\"\\t\")\n",
    "      #to print run_file\n",
    "      #print(\"A\"+str(query_num)+\"\\tQ0\\t\"+str(items[0])+\"\\t\"+str(i+1)+\"\\t\"+str(\"%.02f\"%items[1])+\"\\tSTANDARD\")\n",
    "      i += 1\n",
    "    else:\n",
    "      break\n",
    "################################################################################\n",
    "#Querys\n",
    "#simply add Querys to list \n",
    "querys = [\"espresso\",\"turkish coffee\",\"making a decaffeinated coffee\",\"can I use the same coffee grounds twice?\"]\n",
    "\n",
    "query_num = 0\n",
    "#scans through each query\n",
    "for temp_query in querys:\n",
    "  temp_query = temp_query.lower().strip()\n",
    "  temp_tokens = word_tokenize(re.sub(r'[^\\w\\s]', '', temp_query))\n",
    "  query_tokens = [i for i in temp_tokens if i not in stop_words]\n",
    "  query_num += 1\n",
    "  print(\"Searching: \"+temp_query)\n",
    "  #uses function to search index and print final result\n",
    "  BM25(query_tokens,query_num)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75e7424a951a4d56918a131ca4a3853478fa8f052bd41190059678d885c50057"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
